# üöÄ Spark Multi-Format Data Lake Pipeline with MinIO Integration

A production-ready PySpark pipeline that ingests heterogeneous data files (JSON, CSV, Parquet, DOCX/TXT) into Apache Iceberg tables, then merges the data into existing databases on MinIO object storage. Designed for data lakes where files have unpredictable or varying schemas.

[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PySpark](https://img.shields.io/badge/PySpark-3.5+-orange.svg)](https://spark.apache.org/)
[![PyArrow](https://img.shields.io/badge/PyArrow-14.0+-yellow.svg)](https://arrow.apache.org/docs/python/)
[![Apache Iceberg](https://img.shields.io/badge/Iceberg-1.10+-brightgreen.svg)](https://iceberg.apache.org/)
[![MinIO](https://img.shields.io/badge/MinIO-S3_Compatible-red.svg)](https://min.io/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

## üìã Overview

This pipeline solves a common data engineering challenge: **ingesting multiple files with different schemas** into a unified data lake, then merging them into existing databases on MinIO object storage. Unlike traditional approaches that fail when schemas don't match, this solution:

- ‚úÖ Reads each file individually with its own schema
- ‚úÖ Handles corrupt records gracefully
- ‚úÖ Creates separate Iceberg tables per file
- ‚úÖ Supports JSON, CSV, Parquet, and text formats
- ‚úÖ Merges data into existing databases on MinIO (S3-compatible storage)
- ‚úÖ Handles schema evolution and data versioning
- ‚úÖ Provides comprehensive error handling and logging

## üéØ Use Cases

- **Data Lake Ingestion**: Ingest raw files from multiple sources into MinIO
- **ETL Pipelines**: First stage of data transformation workflows with cloud storage
- **Data Archive**: Store historical data in queryable format on object storage
- **Multi-tenant Systems**: Handle data from different clients/sources with isolation
- **Cloud-Native Data Lakes**: Leverage S3-compatible storage for scalability
- **Hybrid Cloud**: Bridge on-premise and cloud storage solutions
- **Exploratory Data Analysis**: Quick ingestion for analysis with persistent storage

## üèóÔ∏è Architecture
<img width="1759" height="871" alt="image" src="https://github.com/user-attachments/assets/55c34eb0-bfc2-4cd9-9a10-e429857fbed0" />

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Source Files   ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ JSON       ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ CSV        ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Parquet    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ DOCX/TXT   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  PySpark        ‚îÇ
‚îÇ  Individual     ‚îÇ
‚îÇ  File Readers   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  PyArrow        ‚îÇ
‚îÇ  Optimization   ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Partition  ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Batching   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Columnar   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Apache Iceberg ‚îÇ
‚îÇ  Local Tables   ‚îÇ
‚îÇ  (Staging)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  MinIO (S3)     ‚îÇ
‚îÇ  Object Storage ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Buckets    ‚îÇ
‚îÇ      ‚îú‚îÄ‚îÄ Raw    ‚îÇ
‚îÇ      ‚îú‚îÄ‚îÄ Curated‚îÇ
‚îÇ      ‚îî‚îÄ‚îÄ Archive‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
(Optional)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Existing DBs   ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ PostgreSQL ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ MySQL      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ MongoDB    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```
## üì¶ Features

### Multi-Format Support
- **JSON**: Single or multi-line JSON with schema inference
- **CSV**: Handles up to 50,000 columns, custom delimiters
- **Parquet**: Native binary format support
- **Text**: DOCX and TXT files as line-delimited records

### Robust Error Handling
- Corrupt record detection and filtering
- Per-file error isolation (one failure doesn't stop the pipeline)
- Detailed logging and progress reporting
- Empty file detection and skipping

### Performance Optimizations
- Individual file caching with `.persist()`
- Configurable memory settings
- Parallel processing via Spark
- Optimized for large files (tested with 4,450+ columns)

### PyArrow Optimization
- Fast Spark-to-Pandas conversions (5-10x faster)
- Arrow-based columnar data transfers
- Configurable batch size for memory efficiency
- Automatic DataFrame partition optimization
- Fallback support for non-Arrow environments

### Iceberg Integration
- Format version 2 support
- Overwrite and append modes
- Automatic database creation
- Table verification and statistics
- MinIO as Iceberg catalog backend

### MinIO Object Storage
- S3-compatible API integration
- Bucket-based organization
- Versioned data storage
- High availability and scalability
- Cost-effective cloud storage alternative

### Database Merge Operations
- Upsert (insert/update) to PostgreSQL
- Batch inserts to MySQL
- Document merges to MongoDB
- Configurable merge strategies
- Transaction support

## üöÄ Quick Start

### Prerequisites

```bash
# Python 3.8+
python --version

# Java 8 or 11 (required for Spark)
java -version

# Apache Spark 3.5.x

# PyArrow (for optimized conversions)
pip install pyarrow>=16.0.0

# MinIO Server (optional for local testing)
# Download from https://min.io/download
```

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/spark-multi-format-pipeline.git
cd spark-multi-format-pipeline
```

2. **Create virtual environment**
```bash
python -m venv pyspark_env
source pyspark_env/bin/activate  # On Windows: pyspark_env\Scripts\activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Set up MinIO** (for local development)
```bash
# Download MinIO
# Windows
wget https://dl.min.io/server/minio/release/windows-amd64/minio.exe

# Linux/Mac
wget https://dl.min.io/server/minio/release/linux-amd64/minio
chmod +x minio

# Start MinIO server
minio server C:/minio-data --console-address ":9001"

# Default credentials:
# Username: minioadmin
# Password: minioadmin
# Console: http://localhost:9001
```

4. **Download Spark** (if not already installed)
```bash
# Download from https://spark.apache.org/downloads.html
# Extract to a location like C:\spark or /opt/spark
```

5. **Configure MinIO credentials**

Create `.env` file:
```bash
MINIO_ENDPOINT=localhost:9000
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin
MINIO_BUCKET=data-lake
MINIO_SECURE=false
```

### Configuration

1. **Set up directory structure**
```
project/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ json/      # Place JSON files here
‚îÇ   ‚îú‚îÄ‚îÄ csv/       # Place CSV files here
‚îÇ   ‚îú‚îÄ‚îÄ parquet/   # Place Parquet files here
‚îÇ   ‚îî‚îÄ‚îÄ docx/      # Place text files here
‚îú‚îÄ‚îÄ warehouse/     # Iceberg warehouse (auto-created)
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ catalog_config.py                # Catalog session configuration
‚îÇ   ‚îú‚îÄ‚îÄ minio_config.py                  # MiniO session configuration
‚îÇ   ‚îî‚îÄ‚îÄ spark_config.py                  # Spark session configuration
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ arrow_utils.py                 # PyArrow optimization utilities
‚îÇ   ‚îú‚îÄ‚îÄ catalog_manager.py
‚îÇ   ‚îú‚îÄ‚îÄ merge_utils.py                 # Merge from Iceberg to MiniO utilities
‚îÇ   ‚îî‚îÄ‚îÄ spark_write_iceberg.py         # Iceberg write utilities
‚îú‚îÄ‚îÄ schema_manager.py                  # Path configurations
‚îú‚îÄ‚îÄ merge_to_minio.py
‚îî‚îÄ‚îÄ main.py                            # Main pipeline orchestrator
```

2. **Update paths in `schema_manager.py`**
```python
BASE_DIR = Path("C:/Users/Public/Your_Project")
CSV_FILE_PATH = BASE_DIR / "data" / "csv"
JSON_FILE_PATH = BASE_DIR / "data" / "json"
PARQUET_FILE_PATH = BASE_DIR / "data" / "parquet"
DOCX_FILE_PATH = BASE_DIR / "data" / "docx"
```

3. **Configure Spark settings in `config/spark_config.py`**
```python
class SparkConnect:
    def __init__(
        self,
        app_name="DataLakePipeline",
        driver_memory="4g",
        iceberg_warehouse="C:/path/to/warehouse"
    ):
        # Configuration here
```

### Running the Pipeline

```bash
python main.py
```

## üìä Example Output

```
============================================================
=== Reading JSON files ===
============================================================
Found 2 JSON file(s)

  Reading: products.json
    ‚úì 1,234 rows, 15 columns
  Reading: customers.json
    ‚úì 5,678 rows, 8 columns

============================================================
=== Reading CSV files ===
============================================================
Found 2 CSV file(s)

  Reading: data_chunk_001.csv
    ‚úì 50,000 rows, 4,450 columns
  Reading: voice_actors.csv
    ‚úì 890 rows, 3 columns

============================================================
=== LOADING SUMMARY ===
============================================================
Total files successfully loaded: 4

Loaded tables:
  ‚Ä¢ json_products              [JSON   ]    1,234 rows,    15 cols
  ‚Ä¢ json_customers             [JSON   ]    5,678 rows,     8 cols
  ‚Ä¢ csv_data_chunk_001         [CSV    ]   50,000 rows, 4,450 cols
  ‚Ä¢ csv_voice_actors           [CSV    ]      890 rows,     3 cols

============================================================
=== Writing to Iceberg ===
============================================================
Writing 4 table(s) to Iceberg database 'local.iceberg_db'...

‚úÖ All tables written successfully!

============================================================
‚úÖ Pipeline completed successfully!
============================================================
```

## üõ†Ô∏è Project Structure

```
spark-multi-format-pipeline/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ json/      # Place JSON files here
‚îÇ   ‚îú‚îÄ‚îÄ csv/       # Place CSV files here
‚îÇ   ‚îú‚îÄ‚îÄ parquet/   # Place Parquet files here
‚îÇ   ‚îî‚îÄ‚îÄ docx/      # Place text files here
‚îú‚îÄ‚îÄ warehouse/     # Iceberg warehouse (auto-created)
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ catalog_config.py                # Catalog session configuration
‚îÇ   ‚îú‚îÄ‚îÄ minio_config.py                  # MiniO session configuration
‚îÇ   ‚îî‚îÄ‚îÄ spark_config.py                  # Spark session configuration
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ catalog_manager.py
‚îÇ   ‚îú‚îÄ‚îÄ merge_utils.py                 # Merge from Iceberg to MiniO utilities
‚îÇ   ‚îî‚îÄ‚îÄ spark_write_iceberg.py         # Iceberg write utilities
‚îú‚îÄ‚îÄ schema_manager.py                  # Path configurations
‚îú‚îÄ‚îÄ merge_to_minio.py
‚îî‚îÄ‚îÄ main.py                            # Main pipeline orchestrator
‚îú‚îÄ‚îÄ requirements.txt             # Python dependencies
‚îú‚îÄ‚îÄ README.md                    # This file
‚îî‚îÄ‚îÄ LICENSE                      # MIT License
```

## ‚öôÔ∏è Configuration Options

### Memory Settings
Adjust in `main.py`:
```python
driver_memory="4g"      # Increase for larger files
executor_cores=1        # Increase for parallel processing
```

### CSV Options
Modify in `read_csv_file_safely()`:
```python
.option("maxColumns", 50000)     # Max columns to read
.option("delimiter", ",")         # Change delimiter
.option("inferSchema", "true")    # Enable type inference
```

### Iceberg Options
Configure in `write_to_iceberg()`:
```python
mode="overwrite"                  # or "append"
partition_by={"csv_data": ["year", "month"]}  # Partitioning
extra_options={"format-version": "2"}          # Iceberg version
```
### PyArrow Options
Configure in `config/spark_config.py`:
````python
enable_arrow=True              # Enable PyArrow optimization
arrow_batch_size=10000         # Batch size for conversions
````

Functions available in `utils/arrow_utils.py`:
````python
from utils.arrow_utils import (
    spark_to_pandas_arrow,      # Fast Spark ‚Üí Pandas
    pandas_to_spark_arrow,       # Fast Pandas ‚Üí Spark
    optimize_dataframe_for_arrow # Optimize partitions
)
````
````markdown
## üöÄ PyArrow Performance Guide

### When to Use PyArrow

Enable PyArrow (`enable_arrow=True`) when:
- Converting large Spark DataFrames to Pandas (>10K rows)
- Processing DataFrames with many partitions (>200)
- Performing frequent Spark ‚Üî Pandas conversions
- Working with wide tables (many columns)

### Optimization Example
```python
from utils.arrow_utils import optimize_dataframe_for_arrow

# Optimize DataFrame before processing
df_optimized = optimize_dataframe_for_arrow(df)
df_optimized.persist()

# Fast conversion to Pandas
pdf = df_optimized.toPandas()  # Uses Arrow automatically
```

### Performance Tips

- **Batch Size**: Increase `arrow_batch_size` (default 10000) for more memory
- **Partitions**: Arrow works best with 50-200 partitions
- **Memory**: Ensure sufficient driver memory for Arrow buffers
````
## üîç Troubleshooting

### Issue: Out of Memory Error
**Solution**: Increase driver memory
```python
driver_memory="8g"  # or higher
```

### Issue: Corrupt Records in CSV
**Solution**: Pipeline automatically filters corrupt records. Check logs for details.

### Issue: Iceberg Table Not Found
**Solution**: Verify warehouse path and ensure write permissions
```python
# Check warehouse path
WAREHOUSE = "C:/Users/Public/warehouse"  # Must be absolute path
```

### Issue: Schema Inference Slow for Large CSVs
**Solution**: Disable schema inference
```python
.option("inferSchema", "false")  # All columns as strings
```

## üß™ Testing

Test with sample data:

```bash
# Create test files
mkdir -p data/{json,csv,parquet,docx}

# Add sample JSON
echo '{"id": 1, "name": "test"}' > data/json/test.json

# Add sample CSV
echo "col1,col2,col3\n1,2,3\n4,5,6" > data/csv/test.csv

# Run pipeline
python main.py
```

## üìà Performance Benchmarks

Tested on Windows 10, Intel i7, 8GB RAM:

| File Type | File Size | Rows | Columns | Processing Time |
|-----------|-----------|------|---------|-----------------|
| CSV       | 25 MB     | 50K  | 4,450   | ~15 seconds     |
| JSON      | 8 KB      | 10   | 50      | ~2 seconds      |
| Parquet   | 5 MB      | 100K | 20      | ~3 seconds      |
| Text      | 1 MB      | 10K  | 1       | ~1 second       |
````markdown
*Note: Performance improved by ~30-40% with PyArrow optimization enabled for Pandas conversions and large dataset processing.*
````
## ü§ù Contributing

Contributions welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## üìù License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- [Apache Spark](https://spark.apache.org/) - Distributed computing engine
- [Apache Iceberg](https://iceberg.apache.org/) - Table format for data lakes
- [PySpark](https://spark.apache.org/docs/latest/api/python/) - Python API for Spark

## üìß Contact

Your Name - [@vnontop](tuannguyenworkde@gmail.com)

Project Link: [(https://github.com/VNonTOP-DE/-Spark-Multi-Format-Data-Lake-Pipeline)]

## üó∫Ô∏è Roadmap

- [ ] Add support for XML files
- [ ] Implement incremental loading (change data capture)
- [ ] Add data quality checks
- [ ] Create web UI for monitoring
- [ ] Add support for cloud storage (S3, Azure Blob)
- [ ] Implement parallel file processing
- [ ] Add schema evolution support
- [ ] Create Docker container
- [x] Add PyArrow optimization for faster conversions
- [ ] Add support for XML files
- [ ] Implement incremental loading (change data capture)
---

**‚≠ê Star this repo if you find it helpful!**
