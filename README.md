# ğŸš€ Spark Multi-Format Data Lake Pipeline with MinIO Integration

A production-ready PySpark pipeline that ingests heterogeneous data files (JSON, CSV, Parquet, DOCX/TXT) into Apache Iceberg tables, then merges the data into existing databases on MinIO object storage. Designed for data lakes where files have unpredictable or varying schemas.

[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PySpark](https://img.shields.io/badge/PySpark-3.5+-orange.svg)](https://spark.apache.org/)
[![Apache Iceberg](https://img.shields.io/badge/Iceberg-1.10+-brightgreen.svg)](https://iceberg.apache.org/)
[![MinIO](https://img.shields.io/badge/MinIO-S3_Compatible-red.svg)](https://min.io/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

## ğŸ“‹ Overview

This pipeline solves a common data engineering challenge: **ingesting multiple files with different schemas** into a unified data lake, then merging them into existing databases on MinIO object storage. Unlike traditional approaches that fail when schemas don't match, this solution:

- âœ… Reads each file individually with its own schema
- âœ… Handles corrupt records gracefully
- âœ… Creates separate Iceberg tables per file
- âœ… Supports JSON, CSV, Parquet, and text formats
- âœ… Merges data into existing databases on MinIO (S3-compatible storage)
- âœ… Handles schema evolution and data versioning
- âœ… Provides comprehensive error handling and logging

## ğŸ¯ Use Cases

- **Data Lake Ingestion**: Ingest raw files from multiple sources into MinIO
- **ETL Pipelines**: First stage of data transformation workflows with cloud storage
- **Data Archive**: Store historical data in queryable format on object storage
- **Multi-tenant Systems**: Handle data from different clients/sources with isolation
- **Cloud-Native Data Lakes**: Leverage S3-compatible storage for scalability
- **Hybrid Cloud**: Bridge on-premise and cloud storage solutions
- **Exploratory Data Analysis**: Quick ingestion for analysis with persistent storage

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Source Files   â”‚
â”‚  â”œâ”€â”€ JSON       â”‚
â”‚  â”œâ”€â”€ CSV        â”‚
â”‚  â”œâ”€â”€ Parquet    â”‚
â”‚  â””â”€â”€ DOCX/TXT   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PySpark        â”‚
â”‚  Individual     â”‚
â”‚  File Readers   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Apache Iceberg â”‚
â”‚  Local Tables   â”‚
â”‚  (Staging)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MinIO (S3)     â”‚
â”‚  Object Storage â”‚
â”‚  â””â”€â”€ Buckets    â”‚
â”‚      â”œâ”€â”€ Raw    â”‚
â”‚      â”œâ”€â”€ Curatedâ”‚
â”‚      â””â”€â”€ Archiveâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Existing DBs   â”‚
â”‚  â”œâ”€â”€ PostgreSQL â”‚
â”‚  â”œâ”€â”€ MySQL      â”‚
â”‚  â””â”€â”€ MongoDB    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ Features

### Multi-Format Support
- **JSON**: Single or multi-line JSON with schema inference
- **CSV**: Handles up to 50,000 columns, custom delimiters
- **Parquet**: Native binary format support
- **Text**: DOCX and TXT files as line-delimited records

### Robust Error Handling
- Corrupt record detection and filtering
- Per-file error isolation (one failure doesn't stop the pipeline)
- Detailed logging and progress reporting
- Empty file detection and skipping

### Performance Optimizations
- Individual file caching with `.persist()`
- Configurable memory settings
- Parallel processing via Spark
- Optimized for large files (tested with 4,450+ columns)

### Iceberg Integration
- Format version 2 support
- Overwrite and append modes
- Automatic database creation
- Table verification and statistics
- MinIO as Iceberg catalog backend

### MinIO Object Storage
- S3-compatible API integration
- Bucket-based organization
- Versioned data storage
- High availability and scalability
- Cost-effective cloud storage alternative

### Database Merge Operations
- Upsert (insert/update) to PostgreSQL
- Batch inserts to MySQL
- Document merges to MongoDB
- Configurable merge strategies
- Transaction support

## ğŸš€ Quick Start

### Prerequisites

```bash
# Python 3.8+
python --version

# Java 8 or 11 (required for Spark)
java -version

# Apache Spark 3.5.x

# MinIO Server (optional for local testing)
# Download from https://min.io/download
```

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/spark-multi-format-pipeline.git
cd spark-multi-format-pipeline
```

2. **Create virtual environment**
```bash
python -m venv pyspark_env
source pyspark_env/bin/activate  # On Windows: pyspark_env\Scripts\activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Set up MinIO** (for local development)
```bash
# Download MinIO
# Windows
wget https://dl.min.io/server/minio/release/windows-amd64/minio.exe

# Linux/Mac
wget https://dl.min.io/server/minio/release/linux-amd64/minio
chmod +x minio

# Start MinIO server
minio server C:/minio-data --console-address ":9001"

# Default credentials:
# Username: minioadmin
# Password: minioadmin
# Console: http://localhost:9001
```

4. **Download Spark** (if not already installed)
```bash
# Download from https://spark.apache.org/downloads.html
# Extract to a location like C:\spark or /opt/spark
```

5. **Configure MinIO credentials**

Create `.env` file:
```bash
MINIO_ENDPOINT=localhost:9000
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin
MINIO_BUCKET=data-lake
MINIO_SECURE=false
```

### Configuration

1. **Set up directory structure**
```
project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ json/      # Place JSON files here
â”‚   â”œâ”€â”€ csv/       # Place CSV files here
â”‚   â”œâ”€â”€ parquet/   # Place Parquet files here
â”‚   â””â”€â”€ docx/      # Place text files here
â”œâ”€â”€ warehouse/     # Iceberg warehouse (auto-created)
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ catalog_config.py                # Catalog session configuration
â”‚   â”œâ”€â”€ minio_config.py                  # MiniO session configuration
â”‚   â””â”€â”€ spark_config.py                  # Spark session configuration
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ catalog_manager.py
â”‚   â”œâ”€â”€ merge_utils.py                 # Merge from Iceberg to MiniO utilities
â”‚   â””â”€â”€ spark_write_iceberg.py         # Iceberg write utilities
â”œâ”€â”€ schema_manager.py                  # Path configurations
â”œâ”€â”€ merge_to_minio.py
â””â”€â”€ main.py                            # Main pipeline orchestrator
```

2. **Update paths in `schema_manager.py`**
```python
BASE_DIR = Path("C:/Users/Public/Your_Project")
CSV_FILE_PATH = BASE_DIR / "data" / "csv"
JSON_FILE_PATH = BASE_DIR / "data" / "json"
PARQUET_FILE_PATH = BASE_DIR / "data" / "parquet"
DOCX_FILE_PATH = BASE_DIR / "data" / "docx"
```

3. **Configure Spark settings in `config/spark_config.py`**
```python
class SparkConnect:
    def __init__(
        self,
        app_name="DataLakePipeline",
        driver_memory="4g",
        iceberg_warehouse="C:/path/to/warehouse"
    ):
        # Configuration here
```

### Running the Pipeline

```bash
python main.py
```

## ğŸ“Š Example Output

```
============================================================
=== Reading JSON files ===
============================================================
Found 2 JSON file(s)

  Reading: products.json
    âœ“ 1,234 rows, 15 columns
  Reading: customers.json
    âœ“ 5,678 rows, 8 columns

============================================================
=== Reading CSV files ===
============================================================
Found 2 CSV file(s)

  Reading: data_chunk_001.csv
    âœ“ 50,000 rows, 4,450 columns
  Reading: voice_actors.csv
    âœ“ 890 rows, 3 columns

============================================================
=== LOADING SUMMARY ===
============================================================
Total files successfully loaded: 4

Loaded tables:
  â€¢ json_products              [JSON   ]    1,234 rows,    15 cols
  â€¢ json_customers             [JSON   ]    5,678 rows,     8 cols
  â€¢ csv_data_chunk_001         [CSV    ]   50,000 rows, 4,450 cols
  â€¢ csv_voice_actors           [CSV    ]      890 rows,     3 cols

============================================================
=== Writing to Iceberg ===
============================================================
Writing 4 table(s) to Iceberg database 'local.iceberg_db'...

âœ… All tables written successfully!

============================================================
âœ… Pipeline completed successfully!
============================================================
```

## ğŸ› ï¸ Project Structure

```
spark-multi-format-pipeline/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ json/      # Place JSON files here
â”‚   â”œâ”€â”€ csv/       # Place CSV files here
â”‚   â”œâ”€â”€ parquet/   # Place Parquet files here
â”‚   â””â”€â”€ docx/      # Place text files here
â”œâ”€â”€ warehouse/     # Iceberg warehouse (auto-created)
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ catalog_config.py                # Catalog session configuration
â”‚   â”œâ”€â”€ minio_config.py                  # MiniO session configuration
â”‚   â””â”€â”€ spark_config.py                  # Spark session configuration
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ catalog_manager.py
â”‚   â”œâ”€â”€ merge_utils.py                 # Merge from Iceberg to MiniO utilities
â”‚   â””â”€â”€ spark_write_iceberg.py         # Iceberg write utilities
â”œâ”€â”€ schema_manager.py                  # Path configurations
â”œâ”€â”€ merge_to_minio.py
â””â”€â”€ main.py                            # Main pipeline orchestrator
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ README.md                    # This file
â””â”€â”€ LICENSE                      # MIT License
```

## âš™ï¸ Configuration Options

### Memory Settings
Adjust in `main.py`:
```python
driver_memory="4g"      # Increase for larger files
executor_cores=1        # Increase for parallel processing
```

### CSV Options
Modify in `read_csv_file_safely()`:
```python
.option("maxColumns", 50000)     # Max columns to read
.option("delimiter", ",")         # Change delimiter
.option("inferSchema", "true")    # Enable type inference
```

### Iceberg Options
Configure in `write_to_iceberg()`:
```python
mode="overwrite"                  # or "append"
partition_by={"csv_data": ["year", "month"]}  # Partitioning
extra_options={"format-version": "2"}          # Iceberg version
```

## ğŸ” Troubleshooting

### Issue: Out of Memory Error
**Solution**: Increase driver memory
```python
driver_memory="8g"  # or higher
```

### Issue: Corrupt Records in CSV
**Solution**: Pipeline automatically filters corrupt records. Check logs for details.

### Issue: Iceberg Table Not Found
**Solution**: Verify warehouse path and ensure write permissions
```python
# Check warehouse path
WAREHOUSE = "C:/Users/Public/warehouse"  # Must be absolute path
```

### Issue: Schema Inference Slow for Large CSVs
**Solution**: Disable schema inference
```python
.option("inferSchema", "false")  # All columns as strings
```

## ğŸ§ª Testing

Test with sample data:

```bash
# Create test files
mkdir -p data/{json,csv,parquet,docx}

# Add sample JSON
echo '{"id": 1, "name": "test"}' > data/json/test.json

# Add sample CSV
echo "col1,col2,col3\n1,2,3\n4,5,6" > data/csv/test.csv

# Run pipeline
python main.py
```

## ğŸ“ˆ Performance Benchmarks

Tested on Windows 10, Intel i7, 16GB RAM:

| File Type | File Size | Rows | Columns | Processing Time |
|-----------|-----------|------|---------|-----------------|
| CSV       | 25 MB     | 50K  | 4,450   | ~15 seconds     |
| JSON      | 8 KB      | 10   | 50      | ~2 seconds      |
| Parquet   | 5 MB      | 100K | 20      | ~3 seconds      |
| Text      | 1 MB      | 10K  | 1       | ~1 second       |

## ğŸ¤ Contributing

Contributions welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [Apache Spark](https://spark.apache.org/) - Distributed computing engine
- [Apache Iceberg](https://iceberg.apache.org/) - Table format for data lakes
- [PySpark](https://spark.apache.org/docs/latest/api/python/) - Python API for Spark

## ğŸ“§ Contact

Your Name - [@vnontop](tuannguyenworkde@gmail.com)

Project Link: [https://github.com/yourusername/spark-multi-format-pipeline](https://github.com/vnontop-DE/-spark-multi-format-pipeline)

## ğŸ—ºï¸ Roadmap

- [ ] Add support for XML files
- [ ] Implement incremental loading (change data capture)
- [ ] Add data quality checks
- [ ] Create web UI for monitoring
- [ ] Add support for cloud storage (S3, Azure Blob)
- [ ] Implement parallel file processing
- [ ] Add schema evolution support
- [ ] Create Docker container

---

**â­ Star this repo if you find it helpful!**
